<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Publications</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Han Zhong</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="publications.html" class="current">Publications</a></div>
<div class="menu-item"><a href="talks.html">Talks</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Publications</h1>
</div>
<p><b>    </b> * denotes equal contribution or alphabetical authorship ordering <br /></p>
<h2>Conference Publications </h2>
<ul>
<li><p><a href="https://arxiv.org/abs/2305.08841">A Theoretical Analysis of Optimistic Proximal Policy Optimization in Linear Markov Decision Processes</a> <br />
Han Zhong, Tong Zhang <br /> 
Conference on Neural Information Processing Systems (NeurIPS) 2023 <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2305.09659">Double Pessimism is Provably Efficient for Distributionally Robust Offline Reinforcement Learning: Generic Algorithm and Robust Partial Coverage</a>  <br /> 
Jose Blanchet*, Miao Lu*, Tong Zhang*, Han Zhong* <br /> 
Conference on Neural Information Processing Systems (NeurIPS) 2023 (short version) <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2305.18258">Maximize to Explore: One Objective Function Fusing Estimation, Planning, and Exploration</a> <br /> 
Zhihan Liu*, Miao Lu*, Wei Xiong*, Han Zhong, Hao Hu, Shenao Zhang, Sirui Zheng, Zhuoran Yang, Zhaoran Wang <br /> 
Conference on Neural Information Processing Systems (NeurIPS) 2023 <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2306.06836">Tackling Heavy-Tailed Rewards in Reinforcement Learning with Function Approximation: Minimax Optimal and Instance-Dependent Regret Bounds</a> <br /> 
Jiayi Huang, Han Zhong, Liwei Wang, Lin F. Yang <br /> 
Conference on Neural Information Processing Systems (NeurIPS) 2023 <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2302.01477">A Reduction-based Framework for Sequential Decision Making with Delayed Feedback</a> <br />
Yunchang Yang*, Han Zhong*, Tianhao Wu*, Bin Liu, Liwei Wang, Simon S. Du <br />
Conference on Neural Information Processing Systems (NeurIPS) 2023 <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2310.19861">Posterior Sampling for Competitive RL: Function Approximation and Partial Observation</a> <br /> 
Shuang Qiu, Ziyu Dai, Han Zhong, Zhaoran Wang, Zhuoran Yang, Tong Zhang <br /> 
Conference on Neural Information Processing Systems (NeurIPS) 2023 <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2210.15598">Provable Sim-to-real Transfer in Continuous Domain with Partial Observations</a> <br /> 
Jiachen Hu*, Han Zhong*, Chi Jin, Liwei Wang <br />
International Conference on Learning Representations (ICLR) 2023 <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2205.15512">Nearly Minimax Optimal Offline Reinforcement Learning with Linear Function Approximation: Single-Agent MDP and Markov Game</a> <br />
Wei Xiong*, Han Zhong*, Chengshuai Shi, Cong Shen, Liwei Wang, Tong Zhang <br />
International Conference on Learning Representations (ICLR) 2023 <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2205.13863">Why Robust Generalization in Deep Learning is Difficult: Perspective of Expressive Power</a> <br />
Binghui Li*, Jikai Jin*, Han Zhong, John E. Hopcroft, Liwei Wang <br />
Conference on Neural Information Processing Systems (NeurIPS) 2022 <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2202.07511">Pessimistic Minimax Value Iteration: Provably Efficient Equilibrium Learning from Offline Datasets</a> <br />
Han Zhong*, Wei Xiong*, Jiyuan Tan*, Liwei Wang, Tong Zhang, Zhaoran Wang, Zhuoran Yang <br />
International Conference on Machine Learning (ICML) 2022 <br /> 
ICLR 2022 Workshop on Gamification and Multiagent Solutions <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2210.01907">A Self-Play Posterior Sampling Algorithm for Zero-Sum Markov Games</a> <br /> 
Wei Xiong, Han Zhong, Chengshuai Shi, Cong Shen, Tong Zhang <br />
International Conference on Machine Learning (ICML) 2022 <br /> 
ICLR 2022 Workshop on Gamification and Multiagent Solutions <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2112.10935">Nearly Optimal Policy Optimization with Stable at Any Time Guarantee</a> <br />
Tianhao Wu*, Yunchang Yang*, Han Zhong*, Liwei Wang, Simon S. Du, Jiantao Jiao <br />
International Conference on Machine Learning (ICML) 2022 <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://aps.arxiv.org/abs/2205.11140">Human-in-the-loop: Provably Efficient Preference-based Reinforcement Learning with General Function Approximation</a> <br /> 
Xiaoyu Chen*, Han Zhong*, Zhuoran Yang, Zhaoran Wang, Liwei Wang <br />
International Conference on Machine Learning (ICML) 2022 <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2106.11692">A Reduction-Based Framework for Conservative Bandits and Reinforcement Learning</a> <br />
Yunchang Yang*, Tianhao Wu*, Han Zhong*, Evrard Garcelon, Matteo Pirotta, Alessandro Lazaric, Liwei Wang, Simon S. Du <br /> 
International Conference on Learning Representations (ICLR) 2022 <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2110.13876">Breaking the Moments Condition Barrier: No-Regret Algorithm for Bandits with Super Heavy-Tailed Payoffs</a> <br />
Han Zhong, Jiayi Huang, Lin F. Yang, Liwei Wang <br /> 
Conference on Neural Information Processing Systems (NeurIPS) 2021 </p>
</li>
</ul>
<h2>Journal Publications </h2>
<ul>
<li><p><a href="https://arxiv.org/abs/2112.13521">Can Reinforcement Learning Find Stackelberg-Nash Equilibria in General-Sum Markov Games with Myopically Rational Followers?</a> <br />
Han Zhong, Zhuoran Yang, Zhaoran Wang, Michael I. Jordan <br /> 
Journal of Machine Learning Research (JMLR) 2023 <br />
ICLR 2022 Workshop on Gamification and Multiagent Solutions </p>
</li>
</ul>
<h2>Preprints</h2>
<ul>
<li><p><a href="https://arxiv.org/abs/2310.12955">Towards Robust Offline Reinforcement Learning under Diverse Data Corruption</a> <br /> 
Rui Yang*, Han Zhong*, Jiawei Xu*, Amy Zhang, Chongjie Zhang, Lei Han, Tong Zhang</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2302.10796">Provably Efficient Exploration in Quantum Reinforcement Learning with Logarithmic Worst-Case Regret</a> <br /> 
Han Zhong*, Jiachen Hu*, Yecheng Xue, Tongyang Li, Liwei Wang <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2211.01962">GEC: A Unified Framework for Interactive Decision Making in MDP, POMDP, and Beyond</a> <br />
Han Zhong*, Wei Xiong*, Sirui Zheng, Liwei Wang, Zhaoran Wang, Zhuoran Yang, Tong Zhang <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2110.08984">Optimistic Policy Optimization is Provably Efficient in Non-stationary MDPs</a> <br /> 
Han Zhong, Zhuoran Yang, Zhaoran Wang, Csaba Szepesv√°ri <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2012.14098">Risk-Sensitive Deep RL: Variance-Constrained Actor-Critic Provably Finds Globally Optimal Policy</a> <br /> 
Han Zhong, Xun Deng, Ethan X. Fang, Zhuoran Yang, Zhaoran Wang, Runze Li  </p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2023-11-16 01:40:33 CST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
