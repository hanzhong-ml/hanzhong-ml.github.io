# jemdoc: menu{MENU}{index.html}, nofooter  
==Han Zhong (钟涵)

~~~
{}{img_left}{photo.jpg}{alt text}{200}{246}
\n \n
Han Zhong \n 
Ph.D. Student \n 
Peking University \n 
Email: hanzhong@stu.pku.edu.cn \n 
[https://scholar.google.com/citations?user=Bk5q_pAAAAAJ&hl=en Google Scholar]



~~~
 
== About Me 

I am a second year Ph.D. student at Peking University, where I am fortunate to be advised by Professor [http://www.liweiwang-pku.com Liwei Wang]. Before that, I obtained bachelor's degree in Mathematics from University of Science and Technology of China (USTC). Now I am a visiting student at Hong Kong University of Science and Technology (HKUST), working with Professor [http://tongzhang-ml.org/ Tong Zhang]. 


I work on machine learning (reinforcement learning theory in particular). My current research interests include:
\n

- Theoretical foundations of reinforcement learning, e.g., [https://arxiv.org/abs/2211.01962 a unified understanding of interactive decision making in MDP, POMDP, and PSR]. 

- Multiagent reinforcement learning, e.g., equilibria-finding in [https://arxiv.org/abs/2202.07511 offline Markov games] and [https://arxiv.org/abs/2112.13521 Stackelberg games].


- Robustness in machine learning, e.g., [https://arxiv.org/abs/2205.13863 robust generalization in deep learning], and [https://arxiv.org/abs/2210.15598 sim-to-real transfer via robust adversarial training].

- Quantum machine learning, e.g., [https://arxiv.org/abs/2302.10796 quantum reinforcement learning].


If you are interested in collaborating with me or want to have a chat, feel free to contact me through [hanzhong@stu.pku.edu.cn email] or [Wechat.jpeg WeChat].









== Papers

*    * * denotes equal contribution or alphabetical authorship ordering \n
\n

- [https://arxiv.org/abs/2302.10796 Provably Efficient Exploration in Quantum Reinforcement Learning with Logarithmic Worst-Case Regret] \n 
*Han Zhong\**, Jiachen Hu\*, Yecheng Xue, Tongyang Li, Liwei Wang \n 
 ArXiv Preprint \n 

- [https://arxiv.org/abs/2302.01477 A Reduction-based Framework for Sequential Decision Making with Delayed Feedback] \n
Yunchang Yang\*, *Han Zhong\**, Tianhao Wu\*, Bin Liu, Liwei Wang, Simon S. Du \n
 ArXiv Preprint \n 

- [https://arxiv.org/abs/2211.01962 GEC: A Unified Framework for Interactive Decision Making in MDP, POMDP, and Beyond] \n
*Han Zhong\**, Wei Xiong\*, Sirui Zheng, Liwei Wang, Zhaoran Wang, Zhuoran Yang, Tong Zhang \n
 ArXiv Preprint \n 

- [https://arxiv.org/abs/2210.15598 Provable Sim-to-real Transfer in Continuous Domain with Partial Observations] \n 
Jiachen Hu\*, *Han Zhong\**, Chi Jin, Liwei Wang \n
International Conference on Learning Representations (ICLR) 2023 \n 


- [https://arxiv.org/abs/2205.15512 Nearly Minimax Optimal Offline Reinforcement Learning with Linear Function Approximation: Single-Agent MDP and Markov Game] \n
Wei Xiong\*, *Han Zhong\**, Chengshuai Shi, Cong Shen, Liwei Wang, Tong Zhang \n
 International Conference on Learning Representations (ICLR) 2023 \n 


- [https://arxiv.org/abs/2112.13521 Can Reinforcement Learning Find Stackelberg-Nash Equilibria in General-Sum Markov Games with Myopic Followers?] \n
*Han Zhong*, Zhuoran Yang, Zhaoran Wang, Michael I. Jordan \n 
Journal of Machine Learning Research (JMLR) 2023 \n
ICLR 2022 Workshop on Gamification and Multiagent Solutions \n

- [https://arxiv.org/abs/2205.13863 Why Robust Generalization in Deep Learning is Difficult: Perspective of Expressive Power] \n
Binghui Li\*, Jikai Jin\*, *Han Zhong*, John E. Hopcroft, Liwei Wang \n
  Conference on Neural Information Processing Systems (NeurIPS) 2022 \n 


- [https://arxiv.org/abs/2202.07511 Pessimistic Minimax Value Iteration: Provably Efficient Equilibrium Learning from Offline Datasets] \n
 *Han Zhong\**, Wei Xiong\*, Jiyuan Tan\*, Liwei Wang, Tong Zhang, Zhaoran Wang, Zhuoran Yang \n
    International Conference on Machine Learning (ICML) 2022 \n 
     ICLR 2022 Workshop on Gamification and Multiagent Solutions \n


- [https://arxiv.org/abs/2210.01907  A Self-Play Posterior Sampling Algorithm for Zero-Sum Markov Games] \n 
Wei Xiong, *Han Zhong*, Chengshuai Shi, Cong Shen, Tong Zhang \n
 International Conference on Machine Learning (ICML) 2022 \n 
     ICLR 2022 Workshop on Gamification and Multiagent Solutions \n

- [https://arxiv.org/abs/2112.10935 Nearly Optimal Policy Optimization with Stable at Any Time Guarantee] \n
Tianhao Wu\*, Yunchang Yang\*, *Han Zhong\**, Liwei Wang, Simon S. Du, Jiantao Jiao \n
International Conference on Machine Learning (ICML) 2022 \n 

- [https://aps.arxiv.org/abs/2205.11140 Human-in-the-loop: Provably Efficient Preference-based Reinforcement Learning with General Function Approximation] \n 
Xiaoyu Chen\*, *Han Zhong\**, Zhuoran Yang, Zhaoran Wang, Liwei Wang \n
International Conference on Machine Learning (ICML) 2022 \n 
  

- [https://arxiv.org/abs/2110.08984 Optimistic Policy Optimization is Provably Efficient in Non-stationary MDPs] \n 
*Han Zhong*, Zhuoran Yang, Zhaoran Wang, Csaba Szepesvári \n 
Arxiv Preprint \n 

- [https://arxiv.org/abs/2106.11692 A Reduction-Based Framework for Conservative Bandits and Reinforcement Learning] \n
Yunchang Yang\*, Tianhao Wu\*, *Han Zhong\**, Evrard Garcelon, Matteo Pirotta, Alessandro Lazaric, Liwei Wang, Simon S. Du \n 
International Conference on Learning Representations (ICLR) 2022 \n 

- [https://arxiv.org/abs/2110.13876 Breaking the Moments Condition Barrier: No-Regret Algorithm for Bandits with Super Heavy-Tailed Payoffs] \n
*Han Zhong*, Jiayi Huang, Lin F. Yang, Liwei Wang \n 
 Conference on Neural Information Processing Systems (NeurIPS) 2021

- [https://arxiv.org/abs/2012.14098 Risk-Sensitive Deep RL: Variance-Constrained Actor-Critic Provably Finds Globally Optimal Policy] \n 
*Han Zhong*, Ethan X. Fang, Zhuoran Yang, Zhaoran Wang \n 
Arxiv Preprint \n 






