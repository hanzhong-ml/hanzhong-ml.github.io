<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Han Zhong (钟涵)</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Han Zhong</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Han Zhong (钟涵)</h1>
</div>
<table class="imgtable"><tr><td>
<img src="photo.jpg" alt="alt text" width="200px" height="246px" />&nbsp;</td>
<td align="left"><p><br /> <br />
Han Zhong <br /> 
Ph.D. Student <br /> 
Peking University <br /> 
Email: hanzhong@stu.pku.edu.cn <br /> 
<a href="https://scholar.google.com/citations?user=Bk5q_pAAAAAJ&amp;hl=en">Google Scholar</a></p>
</td></tr></table>
<h2>About Me </h2>
<p>I am a second year Ph.D. student at Peking University, where I am fortunate to be advised by Professor <a href="http://www.liweiwang-pku.com">Liwei Wang</a>. Before that, I obtained bachelor's degree in Mathematics from University of Science and Technology of China (USTC). Now I am a visiting student at Hong Kong University of Science and Technology (HKUST), working with Professor <a href="http://tongzhang-ml.org/">Tong Zhang</a>. </p>
<p>I work on machine learning, especially reinforcement learning theory. If you are interested in collaborating with me or want to have a chat, feel free to contact me through <a href="mailto:hanzhong@stu.pku.edu.cn">email</a> or <a href="Wechat.jpeg">WeChat</a>.</p>
<h2>Selected Papers</h2>
<p><b>    </b> * denotes equal contribution or alphabetical authorship ordering <br />
<br /></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2305.08841">A Theoretical Analysis of Optimistic Proximal Policy Optimization in Linear Markov Decision Processes</a> <br />
Han Zhong, Tong Zhang <br /> 
ArXiv Preprint <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2305.09659">Double Pessimism is Provably Efficient for Distributionally Robust Offline Reinforcement Learning: Generic Algorithm and Robust Partial Coverage</a>  <br /> 
Jose Blanchet*, Miao Lu*, Tong Zhang*, Han Zhong* <br /> 
ArXiv Preprint <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2302.10796">Provably Efficient Exploration in Quantum Reinforcement Learning with Logarithmic Worst-Case Regret</a> <br /> 
Han Zhong*, Jiachen Hu*, Yecheng Xue, Tongyang Li, Liwei Wang <br /> 
ArXiv Preprint <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2211.01962">GEC: A Unified Framework for Interactive Decision Making in MDP, POMDP, and Beyond</a> <br />
Han Zhong*, Wei Xiong*, Sirui Zheng, Liwei Wang, Zhaoran Wang, Zhuoran Yang, Tong Zhang <br />
ArXiv Preprint <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2210.15598">Provable Sim-to-real Transfer in Continuous Domain with Partial Observations</a> <br /> 
Jiachen Hu*, Han Zhong*, Chi Jin, Liwei Wang <br />
International Conference on Learning Representations (ICLR) 2023 <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2112.13521">Can Reinforcement Learning Find Stackelberg-Nash Equilibria in General-Sum Markov Games with Myopic Followers?</a> <br />
Han Zhong, Zhuoran Yang, Zhaoran Wang, Michael I. Jordan <br /> 
Journal of Machine Learning Research (JMLR) 2023 <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2205.13863">Why Robust Generalization in Deep Learning is Difficult: Perspective of Expressive Power</a> <br />
Binghui Li*, Jikai Jin*, Han Zhong, John E. Hopcroft, Liwei Wang <br />
Conference on Neural Information Processing Systems (NeurIPS) 2022 <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2202.07511">Pessimistic Minimax Value Iteration: Provably Efficient Equilibrium Learning from Offline Datasets</a> <br />
Han Zhong*, Wei Xiong*, Jiyuan Tan*, Liwei Wang, Tong Zhang, Zhaoran Wang, Zhuoran Yang <br />
International Conference on Machine Learning (ICML) 2022 <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2112.10935">Nearly Optimal Policy Optimization with Stable at Any Time Guarantee</a> <br />
Tianhao Wu*, Yunchang Yang*, Han Zhong*, Liwei Wang, Simon S. Du, Jiantao Jiao <br />
International Conference on Machine Learning (ICML) 2022</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2023-05-19 21:35:03 CST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
