<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Han Zhong (钟涵)</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Han Zhong</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-item"><a href="talks.html">Talks</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Han Zhong (钟涵)</h1>
</div>
<table class="imgtable"><tr><td>
<img src="photo.jpg" alt="alt text" width="200px" height="246px" />&nbsp;</td>
<td align="left"><p><br /> <br />
Han Zhong <br /> 
Ph.D. Student <br /> 
Peking University <br /> 
Email: hanzhong@stu.pku.edu.cn <br /> 
<a href="https://scholar.google.com/citations?user=Bk5q_pAAAAAJ&amp;hl=en">Google Scholar</a> / <a href="https://twitter.com/han_zhong1">Twitter</a> / <a href="Wechat.jpeg">WeChat</a></p>
</td></tr></table>
<h2>About Me </h2>
<p>I am a Ph.D. student at Peking University, where I am fortunate to be advised by Professor <a href="http://www.liweiwang-pku.com">Liwei Wang</a>. Before that, I obtained a bachelor's degree in Mathematics from University of Science and Technology of China (USTC). Additionally, I had the privilege of conducting research at both the Hong Kong University of Science and Technology (HKUST), where I collaborated with Professor <a href="http://tongzhang-ml.org/">Tong Zhang</a>, and at Microsoft Research Asia (MSRA), where I had the opportunity to work with Doctor <a href="https://www.microsoft.com/en-us/research/people/weic/">Wei Chen</a>. This fall, I am a visiting student at Northwestern University, hosted by Professor <a href="https://zhaoranwang.github.io/">Zhaoran Wang</a>.</p>
<p>I work on machine learning. The primary goal of my research is to design provably efficient and practical machine learning algorithms, particularly in the context of interactive decision-making problems. To achieve this goal, my recent researches focus on reinforcement learning theory. Currently, I am also interested in exploring the role of reinforcement learning in foundation models (such as aligning large language models with RLHF). If you share common interests and would like to explore collaboration or simply have a discussion, feel free to contact me.</p>
<h2>Selected Publications</h2>
<p><tt>Theoretical Foundation of Interactive Decision Making:</tt> We propose a unified framework, GEC, to study the statistical complexity of interactive decision making. We also reveal a potential representation complexity hierarchy among different reinforcement learning paradigms, including model-based RL, policy-based RL, and value-based RL.</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2312.17248">Rethinking Model-based, Policy-based, and Value-based Reinforcement Learning via the Lens of Representation Complexity</a> <br /> 
(α-β order) Guhao Feng, Han Zhong  <br /> 
Conference on Neural Information Processing Systems (NeurIPS) 2024 <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2211.01962">GEC: A Unified Framework for Interactive Decision Making in MDP, POMDP, and Beyond</a> <br />
Han Zhong*, Wei Xiong*, Sirui Zheng, Liwei Wang, Zhaoran Wang, Zhuoran Yang, Tong Zhang <br /></p>
</li>
</ul>
<p><tt>Reinforcement Learning from Human Feedback:</tt> We provide the first theoretical result for RLHF with function approximation. We also initialize the studies on RLHF with KL-constraint, under both sentence-wise bandit and token-wise MDP frameworks. Our theoretical insights lead to the development of iterative learning in RLHF and Reinforced Token Optimization (RTO).</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2404.18922">DPO Meets PPO: Reinforced Token Optimization for RLHF</a> <br /> 
Han Zhong*, Guhao Feng*, Wei Xiong*, Li Zhao, Di He, Jiang Bian, Liwei Wang <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2312.11456">Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint</a> <br /> 
Wei Xiong*, Hanze Dong*, Chenlu Ye*, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, Tong Zhang <br />
International Conference on Machine Learning (ICML) 2024 <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2205.11140">Human-in-the-loop: Provably Efficient Preference-based Reinforcement Learning with General Function Approximation</a> <br /> 
Xiaoyu Chen*, Han Zhong*, Zhuoran Yang, Zhaoran Wang, Liwei Wang <br />
International Conference on Machine Learning (ICML) 2022 <br /> </p>
</li>
</ul>
<p><tt>Multi-Agent Reinforcement Learning:</tt> We develope the first line of efficient equilibrium-finding algorithms for offline Markov games and Stackelberg Markov games.</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2112.13521">Can Reinforcement Learning Find Stackelberg-Nash Equilibria in General-Sum Markov Games with Myopically Rational Followers?</a> <br />
Han Zhong, Zhuoran Yang, Zhaoran Wang, Michael I. Jordan <br /> 
Journal of Machine Learning Research (JMLR) 2023 <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2202.07511">Pessimistic Minimax Value Iteration: Provably Efficient Equilibrium Learning from Offline Datasets</a> <br />
Han Zhong*, Wei Xiong*, Jiyuan Tan*, Liwei Wang, Tong Zhang, Zhaoran Wang, Zhuoran Yang <br />
International Conference on Machine Learning (ICML) 2022 <br /> </p>
</li>
</ul>
<p><tt>Robust Machine Learning:</tt> We provide a comprehensive study of distributionally robust RL, exploring its role in reducing sim-to-real gaps and investigating sample-efficient learning in online and offline settings.</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2404.03578">Distributionally Robust Reinforcement Learning with Interactive Data Collection: Fundamental Hardness and Near-Optimal Algorithm</a> <br /> 
Miao Lu*, Han Zhong*, Tong Zhang, Jose Blanchet <br /> 
Conference on Neural Information Processing Systems (NeurIPS) 2024 <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2305.09659">Double Pessimism is Provably Efficient for Distributionally Robust Offline Reinforcement Learning: Generic Algorithm and Robust Partial Coverage</a>  <br /> 
(α-β order) Jose Blanchet, Miao Lu, Tong Zhang, Han Zhong <br /> 
Conference on Neural Information Processing Systems (NeurIPS) 2023 <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2210.15598">Provable Sim-to-real Transfer in Continuous Domain with Partial Observations</a> <br /> 
Jiachen Hu*, Han Zhong*, Chi Jin, Liwei Wang <br />
International Conference on Learning Representations (ICLR) 2023 <br /> </p>
</li>
</ul>
<p><tt>Policy Optimization:</tt> We provide theoretical guarantees for policy optimization algorithms, especially optimistic proximal policy optimization (PPO). </p>
<ul>
<li><p><a href="https://arxiv.org/abs/2305.08841">A Theoretical Analysis of Optimistic Proximal Policy Optimization in Linear Markov Decision Processes</a> <br />
Han Zhong, Tong Zhang <br /> 
Conference on Neural Information Processing Systems (NeurIPS) 2023 <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2112.10935">Nearly Optimal Policy Optimization with Stable at Any Time Guarantee</a> <br />
Tianhao Wu*, Yunchang Yang*, Han Zhong*, Liwei Wang, Simon S. Du, Jiantao Jiao <br />
International Conference on Machine Learning (ICML) 2022 </p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2024-10-02 01:31:28 CDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
