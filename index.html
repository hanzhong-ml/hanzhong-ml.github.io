<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Han Zhong (钟涵)</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Han Zhong</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Han Zhong (钟涵)</h1>
</div>
<table class="imgtable"><tr><td>
<img src="photo.jpg" alt="alt text" width="200px" height="246px" />&nbsp;</td>
<td align="left"><p><br /> <br />
Han Zhong <br /> 
Ph.D. Student <br /> 
Peking University <br /> 
Email: hanzhong@stu.pku.edu.cn <br /> 
<a href="https://scholar.google.com/citations?user=Bk5q_pAAAAAJ&amp;hl=en">Google Scholar</a></p>
</td></tr></table>
<h2>About Me </h2>
<p>I am a second year Ph.D. student in Peking University, where I am fortunate to be advised by Professor <a href="http://www.liweiwang-pku.com">Liwei Wang</a>. <br /></p>
<p>My current research interests focus on machine learning theory (reinforcement learning theory in particular). </p>
<p>If you are interested in collaborating with me or want to have a chat, feel free to contact me through <a href="mailto:hanzhong@stu.pku.edu.cn">email</a> or <a href="Wechat.jpeg">WeChat</a>.</p>
<h2>Papers</h2>
<p><b>    </b> * denotes equal contribution or alphabetical authorship ordering <br />
<br /></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2205.13863">Why Robust Generalization in Deep Learning is Difficult: Perspective of Expressive Power</a> <br />
Binghui Li*, Jikai Jin*, <b>Han Zhong</b>, John E. Hopcroft, Liwei Wang <br />
ArXiv, Preprint <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2205.15512">Nearly Minimax Optimal Offline Reinforcement Learning with Linear Function Approximation: Single-Agent MDP and Markov Game</a> <br />
Wei Xiong*, <b>Han Zhong*</b>, Chengshuai Shi, Cong Shen, Liwei Wang, Tong Zhang <br />
ArXiv, Preprint <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2202.07511">Pessimistic Minimax Value Iteration: Provably Efficient Equilibrium Learning from Offline Datasets</a> <br />
<b>Han Zhong*</b>, Wei Xiong*, Jiyuan Tan*, Liwei Wang, Tong Zhang, Zhaoran Wang, Zhuoran Yang <br />
International Conference on Machine Learning (ICML) 2022 <br /> 
ICLR 2022 Workshop on Gamification and Multiagent Solutions <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://proceedings.mlr.press/v162/xiong22b.html">A Self-Play Posterior Sampling Algorithm for Zero-Sum Markov Games</a> <br /> 
Wei Xiong, <b>Han Zhong</b>, Chengshuai Shi, Cong Shen, Tong Zhang <br />
International Conference on Machine Learning (ICML) 2022 <br /> 
ICLR 2022 Workshop on Gamification and Multiagent Solutions <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2112.10935">Nearly Optimal Policy Optimization with Stable at Any Time Guarantee</a> <br />
Tianhao Wu*, Yunchang Yang*, <b>Han Zhong*</b>, Liwei Wang, Simon S. Du, Jiantao Jiao <br />
International Conference on Machine Learning (ICML) 2022 <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://aps.arxiv.org/abs/2205.11140">Human-in-the-loop: Provably Efficient Preference-based Reinforcement Learning with General Function Approximation</a> <br /> 
Xiaoyu Chen*, <b>Han Zhong*</b>, Zhuoran Yang, Zhaoran Wang, Liwei Wang <br />
International Conference on Machine Learning (ICML) 2022 <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2112.13521">Can Reinforcement Learning Find Stackelberg-Nash Equilibria in General-Sum Markov Games with Myopic Followers?</a> <br />
<b>Han Zhong</b>, Zhuoran Yang, Zhaoran Wang, Michael I. Jordan <br /> 
ICLR 2022 Workshop on Gamification and Multiagent Solutions <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2110.08984">Optimistic Policy Optimization is Provably Efficient in Non-stationary MDPs</a> <br /> 
<b>Han Zhong</b>, Zhuoran Yang, Zhaoran Wang, Csaba Szepesvári <br /> 
Arxiv, Preprint <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2106.11692">A Reduction-Based Framework for Conservative Bandits and Reinforcement Learning</a> <br />
Yunchang Yang*, Tianhao Wu*, <b>Han Zhong*</b>, Evrard Garcelon, Matteo Pirotta, Alessandro Lazaric, Liwei Wang, Simon S. Du <br /> 
International Conference on Learning Representations (ICLR) 2022 <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2110.13876">Breaking the Moments Condition Barrier: No-Regret Algorithm for Bandits with Super Heavy-Tailed Payoffs</a> <br />
<b>Han Zhong</b>, Jiayi Huang, Lin F. Yang, Liwei Wang <br /> 
Neural Information Processing Systems (NeurIPS) 2021</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2012.14098">Risk-Sensitive Deep RL: Variance-Constrained Actor-Critic Provably Finds Globally Optimal Policy</a> <br /> 
<b>Han Zhong</b>, Ethan X. Fang, Zhuoran Yang, Zhaoran Wang <br /> 
Arxiv, Preprint <br /> </p>
</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
