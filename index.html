<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Han Zhong (钟涵)</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Han Zhong</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-item"><a href="talks.html">Talks</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Han Zhong (钟涵)</h1>
</div>
<table class="imgtable"><tr><td>
<img src="photo.jpg" alt="alt text" width="200px" height="246px" />&nbsp;</td>
<td align="left"><p><br /> <br />
Han Zhong <br /> 
Ph.D. Student <br /> 
Peking University <br /> 
Email: hanzhong@stu.pku.edu.cn <br /> 
<a href="https://scholar.google.com/citations?user=Bk5q_pAAAAAJ&amp;hl=en">Google Scholar</a> / <a href="https://twitter.com/han_zhong1">Twitter</a> / <a href="Wechat.jpeg">WeChat</a></p>
</td></tr></table>
<h2>About Me </h2>
<p>I am a third-year Ph.D. student at Peking University, where I am fortunate to be advised by Professor <a href="http://www.liweiwang-pku.com">Liwei Wang</a>. Before that, I obtained a bachelor's degree in Mathematics from University of Science and Technology of China (USTC). Additionally, I had the privilege of conducting research at both the Hong Kong University of Science and Technology (HKUST), where I collaborated with Professor <a href="http://tongzhang-ml.org/">Tong Zhang</a>, and at Microsoft Research Asia (MSRA), where I had the opportunity to work with Doctor <a href="https://www.microsoft.com/en-us/research/people/weic/">Wei Chen</a>.</p>
<p>I work on machine learning. The primary goal of my research is to design provably efficient and practical machine learning algorithms, particularly in the context of interactive decision-making problems. To achieve this goal, my recent researches focus on reinforcement learning theory. Currently, I am also interested in <a href="https://arxiv.org/abs/2205.11140">RLHF</a> (for LLMs) and foundation models (for decision-making problems). If you share common interests and would like to explore collaboration or simply have a discussion, feel free to contact me.</p>
<h2>Selected Publications</h2>
<p><tt>Theoretical Foundation of Interactive Decision Making:</tt> We propose a unified framework, GEC, to study the statistical complexity of interactive decision making. We also reveal a potential representation complexity hierarchy among different reinforcement learning paradigms, including model-based RL, policy-based RL, and value-based RL.</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2312.17248">Rethinking Model-based, Policy-based, and Value-based Reinforcement Learning via the Lens of Representation Complexity</a> <br /> 
Guhao Feng, Han Zhong (alphabetical order) <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2305.18258">Maximize to Explore: One Objective Function Fusing Estimation, Planning, and Exploration</a> <br /> 
Zhihan Liu*, Miao Lu*, Wei Xiong*, Han Zhong, Hao Hu, Shenao Zhang, Sirui Zheng, Zhuoran Yang, Zhaoran Wang <br /> 
Conference on Neural Information Processing Systems (NeurIPS) 2023</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2211.01962">GEC: A Unified Framework for Interactive Decision Making in MDP, POMDP, and Beyond</a> <br />
Han Zhong*, Wei Xiong*, Sirui Zheng, Liwei Wang, Zhaoran Wang, Zhuoran Yang, Tong Zhang <br /></p>
</li>
</ul>
<p><tt>Multi-Agent Reinforcement Learning:</tt> We develope the first line of efficient equilibrium-finding algorithms for offline Markov games and Stackelberg Markov games.</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2112.13521">Can Reinforcement Learning Find Stackelberg-Nash Equilibria in General-Sum Markov Games with Myopically Rational Followers?</a> <br />
Han Zhong, Zhuoran Yang, Zhaoran Wang, Michael I. Jordan <br /> 
Journal of Machine Learning Research (JMLR) 2023 <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2202.07511">Pessimistic Minimax Value Iteration: Provably Efficient Equilibrium Learning from Offline Datasets</a> <br />
Han Zhong*, Wei Xiong*, Jiyuan Tan*, Liwei Wang, Tong Zhang, Zhaoran Wang, Zhuoran Yang <br />
International Conference on Machine Learning (ICML) 2022 <br /> </p>
</li>
</ul>
<p><tt>Robust Machine Learning:</tt> We focus on understanding distributionally robust reinforcement learning and robust generalization in deep learning.</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2305.09659">Double Pessimism is Provably Efficient for Distributionally Robust Offline Reinforcement Learning: Generic Algorithm and Robust Partial Coverage</a>  <br /> 
Jose Blanchet, Miao Lu, Tong Zhang, Han Zhong (alphabetical order) <br /> 
Conference on Neural Information Processing Systems (NeurIPS) 2023 <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2210.15598">Provable Sim-to-real Transfer in Continuous Domain with Partial Observations</a> <br /> 
Jiachen Hu*, Han Zhong*, Chi Jin, Liwei Wang <br />
International Conference on Learning Representations (ICLR) 2023 <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2205.13863">Why Robust Generalization in Deep Learning is Difficult: Perspective of Expressive Power</a> <br />
Binghui Li*, Jikai Jin*, Han Zhong, John E. Hopcroft, Liwei Wang <br />
Conference on Neural Information Processing Systems (NeurIPS) 2022 <br /> </p>
</li>
</ul>
<p><tt>Policy Optimization:</tt> We provide theoretical guarantees for policy optimization algorithms, especially optimistic proximal policy optimization (PPO). </p>
<ul>
<li><p><a href="https://arxiv.org/abs/2305.08841">A Theoretical Analysis of Optimistic Proximal Policy Optimization in Linear Markov Decision Processes</a> <br />
Han Zhong, Tong Zhang <br /> 
Conference on Neural Information Processing Systems (NeurIPS) 2023 <br /> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2112.10935">Nearly Optimal Policy Optimization with Stable at Any Time Guarantee</a> <br />
Tianhao Wu*, Yunchang Yang*, Han Zhong*, Liwei Wang, Simon S. Du, Jiantao Jiao <br />
International Conference on Machine Learning (ICML) 2022 </p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2023-12-31 13:32:05 CST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
